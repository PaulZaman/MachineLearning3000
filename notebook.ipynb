{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning to explain the price of electricity\n",
    "\n",
    "## Authors\n",
    "- [Paul Zamanian](https://www.linkedin.com/in/paul-zamanian-abbassi-899126196)\n",
    "- [Matthieu Vichet](https://www.linkedin.com/in/matthieu-vichet-4b31201a8)\n",
    "- [Benjamin Rossignol](https://www.linkedin.com/in/benjamin-rossignol-4b92521b8)\n",
    "\n",
    "## Summary\n",
    "\n",
    "<span style=\"color:red\">INSERT SUMMARY</span>\n",
    "\n",
    "## Introduction\n",
    "As of 2023, and since 10 years, the energy didn't stop from rising. In France, the soaring cost of electricity is a real problem for the population. The price of electricity is a complex subject, and it is difficult to understand the reasons for this increase. In this project, we will try to explain the price of electricity in France and Germany, and to predict the price of electricity in the future. So, we will try to answer the following questions:\n",
    "- Can we fully explain the price of electricity?\n",
    "- If so, what are the main factors that influence the price of electricity?\n",
    "- What will be the price of electricity in the next few years?\n",
    "\n",
    "We will write all the steps inside this notebook. We will use Python, and the libraries that we will use are Pandas, Matplotlib, Scikit-Learn.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "Here, we have 3 datasets:\n",
    "\n",
    "### Data_X\n",
    "\n",
    "The first one corresponds to the price of electricity in France and Germany on a daily basis of approximately 1 000 days. It contains 35 columns, mainly:\n",
    "- An ID\n",
    "- The day number\n",
    "- The country name. This variable seem to not have any purpose, as all the rows contains the FR and DE values. This means that we will have similar rows, for example:\n",
    "\n",
    "    <span style=\"color:red\">306</span>,207,<span style=\"color:red\">DE</span>,-0.20246037737004818,-0.6330412335287392,...\n",
    "\n",
    "    ...\n",
    "\n",
    "    <span style=\"color:red\">1522</span>,207,<span style=\"color:red\">FR</span>,-0.20246037737004818,-0.6330412335287392,...\n",
    "\n",
    "Here the only difference is the ID and country, the rest of the columns are the same. So, we will regroup the rows and delete the country column.\n",
    "\n",
    "- **x_CONSUMPTION**: The energy consumption per country\n",
    "- **x_y_EXCHANGE**: The energy exchange, how much energy is exchanged between FR and DE\n",
    "- **x_NET_IMPORT/EXPORT**: The energy import/export\n",
    "- **x_GAS/COAL/HYDRO/NUCLEAR/SOLAR/WINDPOW**: The energy production per country (mainly gas, coal, hydro, nuclear, solar, wind)\n",
    "- **DE_LIGNITE**: The lignite production for Germany\n",
    "- **x_RESIDUAL_LOAD**: The electricity consumption after using all renewable energies\n",
    "- **x_RAIN/WIND/TEMP**: The weather data\n",
    "- **GAS_RET/COAL_RET/CARBON_RET**: The gas, coal and carbon prices\n",
    "\n",
    "There are a lot of variables, so focusing on the most important ones will be a good idea. But, what are the most important ones? Perhaps some of them are correlated, so we will have to check that.\n",
    "\n",
    "### Data_Y\n",
    "\n",
    "This data represents the **LEARNING** data. In fact, we have all the IDs from Data_X and the price of electricity for each ID. So, we will have to predict the price of electricity for each ID.\n",
    "\n",
    "### DataNew_X\n",
    "\n",
    "This data is similar to Data_X, but it is for different days. This is the dataset that we will try to predict. This is the **TESTING** data.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Our first step will be to initialize the two datasets, data_x and data_y. Here it is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating the two datasets\n",
    "data_x: pd.DataFrame = pd.read_csv('data/Data_X.csv')\n",
    "data_y: pd.DataFrame = pd.read_csv('data/Data_Y.csv')\n",
    "\n",
    "# Print 5 columns of data\n",
    "print(data_x.head())\n",
    "print(data_y.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that data now contains all the information from Data_X and Data_Y. As said before, we will merge the values from the same day, and delete the COUNTRY column. So we will have only one row per day. To do so, we will remove the duplicates in data_x, keep the IDs that we removed and also remove them from data_y:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we remove the country column\n",
    "data_x = data_x.drop(columns=['COUNTRY'])\n",
    "\n",
    "# if two rows have the same day, we delete one of them\n",
    "data_x = data_x.drop_duplicates(subset=['DAY_ID'], keep='first')\n",
    "\n",
    "# now we have to delete the IDs that we removed from data_x\n",
    "data_y = data_y[data_y['ID'].isin(data_x['ID'])]\n",
    "\n",
    "# we will check by printing the shape of the two datasets\n",
    "print(data_x.shape)\n",
    "print(data_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is correct, we have the same number of rows in data_x and data_y.\n",
    "Our next step is to check if there are any missing values:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(data_x.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, there are a lot of missing values. We will have to deal with them. We decided for this project, to delete the empty lines since we have a lot of data:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_x = data_x.dropna()\n",
    "\n",
    "# we then need to re-delete the values from the data_y dataset\n",
    "data_y = data_y[data_y['ID'].isin(data_x['ID'])]\n",
    "\n",
    "print(data_x.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that there are no missing values, we will plot all the variables to see if there are any outliers.\n",
    "\n",
    "Since the values are very different, we decided to normalize them. First, here are the histograms of all the variables:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hist plot of all the variables\n",
    "data_x.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scales are a bit different, mainly between -5 and 5, but we can see that they look similar.\n",
    "From [this link](https://medium.com/@aziszamcalvin/python-for-data-science-implementing-exploratory-data-analysis-eda-and-k-means-clustering-bcf1d24adc12), we can see that the data distribution can be described using the keywords: normal, skewed, uniform, bimodal, multimodal.\n",
    "Then, the variables follow the following distributions:\n",
    "\n",
    "| Variable         | Distribution        |\n",
    "|------------------|---------------------|\n",
    "| DE_CONSUMPTION   | Unimodal            |\n",
    "| FR_CONSUMPTION   | Skew right          |\n",
    "| DE_FR_EXCHANGE   | Unimodal            |\n",
    "| FR_DE_EXCHANGE   | Unimodal            |\n",
    "| DE_NET_IMPORT    | Skew left           |\n",
    "| DE_NET_EXPORT    | Skew right          |\n",
    "| FR_NET_IMPORT    | Skew right          |\n",
    "| FR_NET_EXPORT    | Skew left           |\n",
    "| DE_GAS           | Skew right          |\n",
    "| FR_GAS           | Multimodal          |\n",
    "| DE_COAL          | Skew right          |\n",
    "| FR_COAL          | Spike right         |\n",
    "| DE_HYDRO         | Skew right          |\n",
    "| FR_HYDRO         | Skew right          |\n",
    "| DE_NUCLEAR       | Multimodal          |\n",
    "| FR_NUCLEAR       | Multimodal          |\n",
    "| DE_SOLAR         | Multimodal          |\n",
    "| FR_SOLAR         | Multimodal          |\n",
    "| DE_WINDPOW       | Skew right          |\n",
    "| FR_WINDPOW       | Skew right          |\n",
    "| DE_LIGNITE       | Skew left           |\n",
    "| DE_RESIDUAL_LOAD | Symmetric, unimodal |\n",
    "| FR_RESIDUAL_LOAD | Skew right          |\n",
    "| DE_RAIN          | Skew right          |\n",
    "| FR_RAIN          | Skew right          |\n",
    "| DE_WIND          | Skew right          |\n",
    "| FR_WIND          | Skew right          |\n",
    "| DE_TEMP          | Symmetric, unimodal |[^1]\n",
    "| FR_TEMP          | Symmetric, unimodal |[^2]\n",
    "\n",
    "[^1]: footnote 1\n",
    "[^2]: footnote 2\n",
    "Now, our two normalization scales are MinMax and Standard. To use standard, it is better to have a normal data distribution. Here, it is not always the case: some of them are close to normal, but some are not. So, to make it easier, we will use MinMax on every variable. Here is the code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# we will use MinMax on every variable except the ID, DAY_ID and COUNTRY (the 3 first columns)\n",
    "# we will store the normalized data in normalized_data\n",
    "\n",
    "scaler: MinMaxScaler = MinMaxScaler()\n",
    "normalized_data: pd.DataFrame = data_x.copy()\n",
    "normalized_data[normalized_data.columns[3:]] = scaler.fit_transform(normalized_data[normalized_data.columns[3:]])\n",
    "\n",
    "# we will check by printing the histograms again\n",
    "normalized_data.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The histograms look similar and the scales are now between 0 and 1.\n",
    "It is also important to see what our goal is. We will plot the content of Data_Y:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# show only TARGET\n",
    "data_y.hist(bins=50, figsize=(2, 2), column='TARGET')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the target variable follows a normal distribution. It is centered and symmetric.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Now that our data is normalized, we will try to split the data to make categories out of the price of electricity. We will **CUT** the data into 5 categories:\n",
    "- **Very Low**: 0 - 0.2\n",
    "- **Low**: 0.2 - 0.4\n",
    "- **Medium**: 0.4 - 0.6\n",
    "- **High**: 0.6 - 0.8\n",
    "- **Very High**: 0.8 - 1\n",
    "\n",
    "We will use the **CUT** function from Pandas to do so:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we will use \"cut\" to cut all columns except the first 2 (ID, DAY_ID). We will store the result in the same dataframe\n",
    "normalized_data[normalized_data.columns[2:]] = normalized_data[normalized_data.columns[2:]].apply(pd.cut, bins=[0, 0.2, 0.4, 0.6, 0.8, 1], labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we will check by printing data_x\n",
    "print(data_x.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}